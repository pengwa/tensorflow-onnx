# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT license.

"""
tf2onnx.rewriter.custom_rnn_rewriter - custom rnn support
"""

from __future__ import division
from __future__ import print_function
import logging
import sys
import traceback
from onnx import onnx_pb
import numpy as np
from tf2onnx.graph import Graph
from tf2onnx.graph_matcher import OpTypePattern, GraphMatcher
from tf2onnx.rewriter.loop_rewriter_base import LoopRewriterBase, Context, TensorArrayProp
from tf2onnx.rewriter.rnn_utils import is_tensor_array_gather_op, is_tensor_array_write_op
from tf2onnx.rewriter.rnn_utils import BodyGraphDict, REWRITER_RESULT, SubGraphMetadata
from tf2onnx.tfonnx import utils


logging.basicConfig(level=logging.INFO)
log = logging.getLogger("tf2onnx.rewriter.custom_rnn_rewriter")


# pylint: disable=missing-docstring,invalid-name,unused-argument,using-constant-test,broad-except,protected-access


class CustomRnnContext(Context):
    def __init__(self):
        super(CustomRnnContext, self).__init__()
        self.other_loop_vars = {}
        self.rnn_scope = None

        self.time_var = None
        self.iteration_var = None


class CustomRnnRewriter(LoopRewriterBase):
    def __init__(self, g):
        super(CustomRnnRewriter, self).__init__(g)

    def create_context(self):
        return CustomRnnContext()

    def run(self):
        log.debug("enter custom rnn rewriter")
        return self.run_internal()

    def _get_rnn_scope_name(self, while_scope_name):
        parts = while_scope_name.split('/')
        rnn_scope = '/'.join(parts[0:-2]) + "/"
        log.debug("found rnn scope %s", rnn_scope)
        return rnn_scope

    def _parse_rnn_loop(self, context):
        # check a while loop is generated by dynamic_rnn or bidirectional_rnn by
        #
        # 1. some patterns in _time_step in dynamic_rnn: tensor array read, tensor array write
        # 2. some patterns in control_flow_ops.while_loop in dynamic_rnn:
        #      cond: time < loop_bound
        #      loop_vars: (time, output_ta, state)
        #      time has name called "time"
        #      iteration_cnt is added by control flow.

        # be noted:
        # 1. iteration counter does not exist in tf1.4 or earlier versions
        # 2. if dynamic_rnn's first input is not consumed, output ta does not exist.
        time_name = context.rnn_scope + "time"
        ta_array_name_prefix = context.rnn_scope + "dynamic_rnn/output_"
        iteration_counter_name = context.while_context_scope + "iteration_counter"

        found_time = False
        is_rnn_out_ta = True
        for enter_name, val in context.loop_variables.items():
            enter_input_node = self.g.get_node_by_output(val.enter_input_id)
            if val.is_tensor_array:
                ta_name = enter_input_node.get_attr("tensor_array_name").s.decode("utf-8")
                if not ta_name.startswith(ta_array_name_prefix):
                    is_rnn_out_ta = False
            elif enter_input_node.name == time_name:
                found_time = True
                context.time_var = val
            elif enter_input_node.name == iteration_counter_name:
                context.iteration_var = val
            else:
                context.other_loop_vars[enter_name] = val

        if not (found_time and is_rnn_out_ta):
            log.debug("this should not be a dynamic_rnn loop, found_time: %s, is_rnn_out_ta: %s",
                      found_time, is_rnn_out_ta)
            return False

        return True

    def need_rewrite(self, context):
        context.rnn_scope = self._get_rnn_scope_name(context.while_context_scope)

        if not self._parse_rnn_loop(context):
            log.debug("skip the loop due to parse_rnn_loop failed")
            return False

        self._parse_time_var(context)

        if not context.input_tas:
            log.debug("this should not be a dynamic_rnn loop, no ta input or output are found")
            return False
        return True

    def rewrite(self, context):
        log.debug("enter rewrite function")
        scan_props = context.loop_properties
        nodes_to_append = []
        for i, state_input in enumerate(scan_props.initial_state_variable_values):
            nodes = self._adapt_scan_sequence_input_or_output("input", state_input, False)
            scan_props.initial_state_variable_values[i] = nodes[-1].output[0]
            nodes_to_append.extend(nodes)

        for i, scan_input in enumerate(scan_props.initial_scan_variable_values):
            nodes = self._adapt_scan_sequence_input_or_output("input", scan_input, False)
            scan_props.initial_scan_variable_values[i] = nodes[-1].output[0]
            nodes_to_append.extend(nodes)

        cell_g_info = context.cell_graph
        scan_body_g = LoopRewriterBase.construct_graph_from_nodes(self.g, cell_g_info.nodes, cell_g_info.outputs)
        loop_body_g_inputs = []
        for input_name in scan_props.state_inputs:
            val = utils.make_onnx_inputs_outputs(input_name, self.g.get_dtype(input_name),
                                                 self.g.get_shape(input_name))
            loop_body_g_inputs.append(val)

        for input_name in scan_props.scan_inputs:
            shape = self.g.get_shape(input_name)
            dtype = self.g.get_dtype(input_name)
            val = utils.make_onnx_inputs_outputs(input_name, dtype, shape)
            loop_body_g_inputs.append(val)

        for i in loop_body_g_inputs:
            scan_body_g.add_model_input(i)

        scan_node = self._create_scan_node(context, scan_props)
        if not scan_node:
            log.error("failed to create scan node during rewrite")
            return REWRITER_RESULT.FAIL

        # set scan output shapes and dtypes
        index = 0
        for state_var_init_val in scan_props.initial_state_variable_values:
            shape = self.g.get_shape(state_var_init_val)
            dtype = self.g.get_dtype(state_var_init_val)
            self.g.set_shape(scan_node.output[index], shape)
            self.g.set_dtype(scan_node.output[index], dtype)
            index += 1

        last_scan_input_shape = self.g.get_shape(scan_node.input[-1])
        batch = last_scan_input_shape[0] # should be 1
        utils.make_sure(batch == 1, "fake batch should be 1")
        time = last_scan_input_shape[1]
        for i in range(len(scan_props.scan_outputs)):
            scan_out_dtype = scan_body_g.get_dtype(scan_props.scan_outputs[i])
            output_shape = scan_body_g.get_shape(scan_props.scan_outputs[i])
            scan_output_shape = [batch, time] + output_shape
            log.debug("scan output [%s] has shape %s, batch:%s, time: %s, cell output shape: %s",
                      scan_props.scan_outputs[i], scan_output_shape, batch, time, output_shape)
            log.debug("_create_scan_node - set scan scan_output shape for %s[%s]:%s",
                      scan_node.name, index, scan_output_shape)
            self.g.set_shape(scan_node.output[index], scan_output_shape)
            self.g.set_dtype(scan_node.output[index], scan_out_dtype)
            index += 1
        # set shapes and dtypes end

        scan_node.set_body_graph_as_attr("body", scan_body_g)
        nodes_to_append.append(scan_node)

        to_append = self._connect_scan_with_output(context, scan_node)
        nodes_to_append.extend(to_append)
        all_nodes = self.g.get_nodes()
        all_nodes.extend(nodes_to_append)
        self.g.set_nodes(all_nodes)

        return REWRITER_RESULT.OK

    def _parse_time_var(self, context):
        time_var = context.time_var
        log.debug("time var %s - enter input id (%s) shape: %s, output (%s) shape: %s", time_var.enter_name,
                  time_var.enter_input_id, self.g.get_shape(time_var.enter_input_id),
                  time_var.switch_true_identity_output_id, self.g.get_shape(time_var.switch_true_identity_output_id))

    def _create_scan_node(self, context, scan_props):
        log.debug("create scan node")
        # here we did not give the sequence_length, because
        # current batch size is 1, not original batch size
        # original seq_length will be used by the loop body of Scan op.
        scan_node = self.g.make_node("Scan", [""] + scan_props.initial_state_variable_values + scan_props.initial_scan_variable_values,
                                     attr={"num_scan_inputs": len(scan_props.scan_inputs)},
                                     output_count=len(scan_props.state_outputs + scan_props.scan_outputs),
                                     skip_conversion=False)
        return scan_node

    def _connect_scan_with_output(self, context, scan_node):
        log.debug("connect scan output with the graph")

        index = 0 # ignore the 1st input (time-iterator)
        nodes_to_append = []
        for var_output_id in context.loop_properties.state_output_exit_ids:
            if var_output_id:
                nodes = self._adapt_scan_sequence_input_or_output("state_output_reshape",
                                                                  scan_node.output[index], True)
                nodes_to_append.extend(nodes)
                self.g.replace_all_inputs(self.g.get_nodes(), var_output_id, nodes[-1].output[0])

            index += 1

        for var_output_id in context.loop_properties.scan_output_exit_ids:
            if var_output_id:
                nodes = self._adapt_scan_sequence_input_or_output("scan_output_reshape",
                                                                  scan_node.output[index], True)
                nodes_to_append.extend(nodes)
                self.g.replace_all_inputs(self.g.get_nodes(), var_output_id, nodes[-1].output[0])
            index += 1

        return nodes_to_append

    def _adapt_scan_sequence_input_or_output(self, target_name, input_id, handle_output=False):
        nodes_to_add = []
        shape_node = self.g.make_node("Shape", [input_id])
        nodes_to_add.append(shape_node)
        inferred_shape = self.g.get_shape(input_id)
        if handle_output is True:
            # handle output:
            # if required dim values don't contain more than one -1,
            # just use a const for Reshape's shape input.
            if inferred_shape is not None and inferred_shape[1:].count(-1) <= 1:
                new_shape_node = self.g.make_const(utils.make_name(target_name + "_target_shape"),
                                                   np.array(inferred_shape[1:], dtype=np.int64))
            else:
                # otherwise, get the dim dynamically, e.g. remove the fake batch size (e.g.1)
                # from [1, time, real-batch, ...]
                origin_shape_node = self.g.make_node("Cast", [shape_node.output[0]],
                                                     {"to": onnx_pb.TensorProto.FLOAT})
                nodes_to_add.append(origin_shape_node)

                sliced_shape_node = self.g.make_node("Slice", [origin_shape_node.output[0]],
                                                     {"axes": [0], "starts": [1], "ends": [sys.maxsize]})
                nodes_to_add.append(sliced_shape_node)

                new_shape_node = self.g.make_node("Cast", [sliced_shape_node.output[0]],
                                                  {"to": onnx_pb.TensorProto.INT64})
                nodes_to_add.append(new_shape_node)

            new_shape = inferred_shape[1:]
        else:
            # handle input:
            if inferred_shape is not None and inferred_shape.count(-1) <= 1:
                new_shape_node = self.g.make_const(utils.make_name(target_name + "_target_shape"),
                                                   np.array([1] + inferred_shape, dtype=np.int64))
            else:
                # add a fake batch size : 1
                fake_batch_size_node = self.g.make_const(utils.make_name(target_name + "_target_shape"),
                                                         np.array([1,], dtype=np.int64))
                new_shape_node = self.g.make_node("Concat",
                                                  [fake_batch_size_node.output[0], shape_node.output[0]],
                                                  attr={"axis": 0})
                nodes_to_add.append(new_shape_node)
            new_shape = [1] + inferred_shape

        reshape_node = self.g.make_node("Reshape", [input_id, new_shape_node.output[0]],
                                        shapes=[new_shape],
                                        dtypes=[self.g.get_dtype(input_id)],
                                        op_name_scope=target_name)
        nodes_to_add.append(reshape_node)
        log.debug("create Reshape for scan output %s, with output shape %s",
                  reshape_node.output[0], new_shape)
        return nodes_to_add

    '''
    # in theory, time var can be a scalar, but in current implementation of runtime, it could not be handled
    # correctly, so we unsqueeze it to a list containing a single element.
    def _adapt_time_var_as_workaround(self, g, context):
        log.debug("_adapt_time_var_as_workaround")
        nodes_to_append = []
        # change time shape to {1} since current Scan does not support
        time_init_node = self._create_unsqueeze_node(self.g, "time_var_init", context.loop_properties.initial_state_variable_values[0])
        nodes_to_append.append(time_init_node)
        context.loop_properties.initial_state_variable_values[0] = time_init_node.output[0]

        nodes_to_subgraph = []
        time_output_node = self._create_unsqueeze_node(g, "time_var_output", context.loop_properties.scan_outputs[0])
        nodes_to_subgraph.append(time_output_node)
        context.loop_properties.scan_outputs[0] = time_output_node.output[0]

        time_input_node = self._create_squeeze_node(g, "time_var_input", var.switch_true_identity_output_id)
        nodes_to_subgraph.append(time_input_node)

        self.g.replace_all_inputs(self.g.get_nodes(), var.switch_true_identity_output_id, time_input_node.output[0])
        self.g.set_shape(var.switch_true_identity_output_id, [1] + self.g.get_shape(var.switch_true_identity_output_id))

        return nodes_to_append

    def _create_unsqueeze_node(self, g, target_name, input_id):
        input_shape = g.get_shape(input_id)
        utils.make_sure(input_shape is not None, input_id + "'s shape is none")
        output_shape = [1] + input_shape
        unsqueeze_node = g.make_node("Unsqueeze", [input_id], attr={"axes": [0]},
                                          shapes=[output_shape], dtypes=[g.get_dtype(input_id)],
                                          op_name_scope=target_name)

        return unsqueeze_node

    def _create_squeeze_node(self, g, target_name, input_id):
        input_shape = g.get_shape(input_id)
        utils.make_sure(input_shape is not None, input_id + "'s shape is none")
        output_shape = list(input_shape)[1:]
        squeeze_node = g.make_node("Squeeze", [input_id], attr={"axes": [0]},
                                        shapes=[output_shape], dtypes=[g.get_dtype(input_id)],
                                        op_name_scope=target_name)

        return squeeze_node
    # end of time var workaround
    '''

class CustomRnnLateRewriter(object):
    def __init__(self, g):
        self.g = g

    def rewrite(self):
        log.debug("enter custom rnn late rewriter")
        nodes = self.g.get_nodes()
        nodes_to_remove = []
        for scan_node in nodes:
            if scan_node.type != "Scan":
                continue
            log.debug("late write for scan node %s", scan_node.name)
            num_scan_inputs = scan_node.get_attr("num_scan_inputs").i
            if not BodyGraphDict.has_body_graph_info(scan_node.name):
                continue

            body_graph_meta = BodyGraphDict.pop_body_graph_info(scan_node.name)
            input_ids = body_graph_meta.input_ids
            if body_graph_meta.other_enter_input_ids:
                input_ids += body_graph_meta.other_enter_input_ids
            output_ids = body_graph_meta.output_ids
            onnx_nodes, _, _ = LoopRewriterBase.find_subgraph(input_ids, output_ids, self.g)

            log.debug("start creating body graph for scan node %s ", scan_node.name)
            const_nodes = [n for n in onnx_nodes if n.type in ("Const", "ConstV2")]
            for n in const_nodes:
                # when set nodes, Const should be removed, they need be replaced as initializers.
                onnx_nodes.remove(n)

            onnx_nodes = set(onnx_nodes)
            nodes_to_remove.extend(onnx_nodes)

            ops = []
            for op in onnx_nodes:
                onnx_op = op.op
                ops.append(onnx_op)

            body_g = Graph(ops, output_shapes=self.g._output_shapes, dtypes=self.g._dtypes)

            log.debug("start preparing body graph inputs nodes")
            temp_nodes = body_g.get_nodes()
            i = 0
            input_count = len(body_graph_meta.input_ids)
            for input_name, init_input_id in zip(body_graph_meta.input_ids, body_graph_meta.initial_input_ids):
                shape = body_g.get_shape(input_name)
                dtype = body_g.get_dtype(input_name)
                if shape is None:
                    shape = self.g.get_shape(init_input_id)
                    if i >= input_count - num_scan_inputs:
                        loop_input_shape = list(shape)[2:]  # delete [1, time,]
                    else:
                        loop_input_shape = list(shape)
                else:
                    loop_input_shape = list(shape)

                val = utils.make_onnx_inputs_outputs(input_name, dtype, loop_input_shape)
                body_g.add_model_input(val)
                i += 1

            log.debug("start preparing body graph outputs nodes")
            new_output_names = []
            for o in body_graph_meta.output_ids:
                # insert identity node, since sometimes we need output same output_id as state_output
                # and scan_out, but ONNX don't allow the same output_id appeared more than once as
                # output node.
                node = body_g.make_node("Identity", inputs=[o], shapes=[body_g.get_shape(o)],
                                        dtypes=[body_g.get_dtype(o)])
                new_output_names.append(node.output[0])
                temp_nodes.append(node)

            body_g.set_nodes(temp_nodes)
            body_g.topological_sort(body_g.get_nodes())

            log.debug("start make graph based on body graph nodes")
            body_g.output_names = new_output_names
            graph = body_g.make_graph("scan body graph", graph_name=scan_node.name + "_body_graph")
            scan_node.set_attr("body", graph)

        # remove nodes in body graph from g
        for n in set(nodes_to_remove):
            if n in nodes:
                nodes.remove(n)

        return nodes
